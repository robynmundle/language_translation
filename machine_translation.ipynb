{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Load Data --> Deciding to do German-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to /home/robyn/nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving corpora: alignment-de-en.txt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('comtrans')\n",
    "from nltk.corpus import comtrans\n",
    "# function to retrieve the corpora\n",
    "def retrieve_corpora(translated_sentences_l1_l2='alignment-de-en.txt'):\n",
    "    print(\"Retrieving corpora: {}\".format(translated_sentences_l1_l2))\n",
    "    als = comtrans.aligned_sents(translated_sentences_l1_l2)\n",
    "    sentences_l1 = [sent.words for sent in als]\n",
    "    sentences_l2 = [sent.mots for sent in als]\n",
    "    return sentences_l1, sentences_l2\n",
    "\n",
    "sen_l1, sen_l2 = retrieve_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "german_sentences = sen_l1\n",
    "# Load French data\n",
    "english_sentences = sen_l2\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German sample 1:  ['Wiederaufnahme', 'der', 'Sitzungsperiode']\n",
      "English sample 1:  ['Resumption', 'of', 'the', 'session']\n",
      "\n",
      "German sample 2:  ['Ich', 'erkläre', 'die', 'am', 'Freitag', ',', 'dem', '17.', 'Dezember', 'unterbrochene', 'Sitzungsperiode', 'des', 'Europäischen', 'Parlaments', 'für', 'wiederaufgenommen', ',', 'wünsche', 'Ihnen', 'nochmals', 'alles', 'Gute', 'zum', 'Jahreswechsel', 'und', 'hoffe', ',', 'daß', 'Sie', 'schöne', 'Ferien', 'hatten', '.']\n",
      "English sample 2:  ['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
      "\n",
      "German sample 3:  ['Wie', 'Sie', 'feststellen', 'konnten', ',', 'ist', 'der', 'gefürchtete', '\"', 'Millenium-Bug', '\"', 'nicht', 'eingetreten', '.', 'Doch', 'sind', 'Bürger', 'einiger', 'unserer', 'Mitgliedstaaten', 'Opfer', 'von', 'schrecklichen', 'Naturkatastrophen', 'geworden', '.']\n",
      "English sample 3:  ['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'\", 'millennium', 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
      "\n",
      "German sample 4:  ['Im', 'Parlament', 'besteht', 'der', 'Wunsch', 'nach', 'einer', 'Aussprache', 'im', 'Verlauf', 'dieser', 'Sitzungsperiode', 'in', 'den', 'nächsten', 'Tagen', '.']\n",
      "English sample 4:  ['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part-session', '.']\n",
      "\n",
      "German sample 5:  ['Ich', 'bitte', 'Sie', ',', 'sich', 'zu', 'einer', 'Schweigeminute', 'zu', 'erheben', '.']\n",
      "English sample 5:  ['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(5):\n",
    "    print('German sample {}:  {}'.format(sample_i + 1, german_sentences[sample_i]))\n",
    "    print('English sample {}:  {}\\n'.format(sample_i + 1, english_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already. \n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666937 German words.\n",
      "36146 unique German words.\n",
      "10 Most common words in the German dataset:\n",
      "\",\" \".\" \"die\" \"der\" \"und\" \"in\" \"zu\" \"den\" \"ist\" \"daß\"\n",
      "\n",
      "710091 English words.\n",
      "19231 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"the\" \".\" \",\" \"of\" \"to\" \"and\" \"in\" \"is\" \"a\" \"that\"\n"
     ]
    }
   ],
   "source": [
    "german_words_counter = collections.Counter([word for sentence in german_sentences for word in sentence])\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence])\n",
    "\n",
    "print('{} German words.'.format(len([word for sentence in german_sentences for word in sentence])))\n",
    "print('{} unique German words.'.format(len(german_words_counter)))\n",
    "print('10 Most common words in the German dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*german_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean\n",
    "In the following step, we want to clean up the tokens. Specifically, we want to tokenize punctuation and lowercase the tokens. We will use the regex module to perform the further splitting tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    regex_splitter = re.compile(\"([!?.,:;$\\\"')( ])\")\n",
    "    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n",
    "    return [w for words in clean_words for w in words if words if w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s). We can turn each word into a number called word ids. A word level model uses word ids that generate text predictions for each word.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `german_sentences` and `english_sentences` in the cell below. Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Dataset Size\n",
    "The next step for this project is filtering the sentences that are too long to be processed. Since our goal is to perform the processing on a local machine, we should limit ourselves to sentences up to N tokens. In this case, we set N=20, in order to be able to train the learner within 24 hours. If you have a powerful machine, feel free to increase that limit. To make the function generic enough, there’s also a lower bound with a default value set to 0, such as an empty token set.\n",
    "\n",
    "The logic of the function is very easy: if the number of tokens for a sentence or its translation is greater than N, then the sentence (in both languages) is removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to minimize processing power, limit sentences to 20 word length -- can drop this later if proven to not take too long to train\n",
    "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
    "    filtered_sentences_l1 = []\n",
    "    filtered_sentences_l2 = []\n",
    "    for i in range(len(sentences_l1)):\n",
    "        if min_len <= len(sentences_l1[i]) <= max_len and min_len <= len(sentences_l2[i]) <= max_len:\n",
    "            filtered_sentences_l1.append(sentences_l1[i])\n",
    "            filtered_sentences_l2.append(sentences_l2[i])\n",
    "    return filtered_sentences_l1, filtered_sentences_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max German sentence length: 20\n",
      "Max English sentence length: 20\n",
      "German vocabulary size: 34213\n",
      "English vocabulary size: 17343\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences (Language 1)\n",
    "    :param y: Label List of sentences (Language 2)\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x = [clean_sentence(s) for s in x]\n",
    "    preprocess_y = [clean_sentence(s) for s in y]\n",
    "    \n",
    "    preprocess_x, x_tk = tokenize(preprocess_x)\n",
    "    preprocess_y, y_tk = tokenize(preprocess_y)\n",
    "\n",
    "    preprocess_x, preprocess_y = filter_sentence_length(preprocess_x, preprocess_y)\n",
    "    \n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_german_sentences, preproc_english_sentences, german_tokenizer, english_tokenizer = preprocess(german_sentences, english_sentences)\n",
    "    \n",
    "max_german_sequence_length = preproc_german_sentences.shape[1]\n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "german_vocab_size = len(german_tokenizer.word_index)\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max German sentence length:\", max_german_sequence_length)\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"German vocabulary size:\", german_vocab_size)\n",
    "print(\"English vocabulary size:\", english_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gap between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = ''\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(input_shape, output_sequence_length, german_vocab_size, english_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"   \n",
    "    # Initialising the RNN\n",
    "    model = Sequential()\n",
    "    # word embedding layer seen before\n",
    "    model.add(Embedding(german_vocab_size, 1024, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 1024, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    # Adding a second LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 512, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    # Adding a third LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 512, return_sequences = True))\n",
    "    #model.add(Dropout(0.2))\n",
    "    # Adding a fourth LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 512, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # hidden dense layer\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    # output dense layer\n",
    "    model.add(TimeDistributed(Dense(english_vocab_size, activation='softmax')))\n",
    "    # Compiling the RNN\n",
    "    model.compile(optimizer = 'adam', loss = sparse_categorical_crossentropy, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 20, 1024)          35035136  \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 20, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 20, 512)           3147776   \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 20, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 20, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 20, 512)           262656    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 20, 17344)         8897472   \n",
      "=================================================================\n",
      "Total params: 59,934,144\n",
      "Trainable params: 59,934,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "237/237 [==============================] - 1429s 6s/step - loss: 5.4895 - accuracy: 0.3387 - val_loss: 4.4296 - val_accuracy: 0.3678\n",
      "Epoch 2/25\n",
      "237/237 [==============================] - 1409s 6s/step - loss: 4.4633 - accuracy: 0.3642 - val_loss: 4.3914 - val_accuracy: 0.3719\n",
      "Epoch 3/25\n",
      "237/237 [==============================] - 1412s 6s/step - loss: 4.3300 - accuracy: 0.3705 - val_loss: 4.2576 - val_accuracy: 0.3785\n",
      "Epoch 4/25\n",
      "237/237 [==============================] - 1412s 6s/step - loss: 4.1839 - accuracy: 0.3801 - val_loss: 4.2413 - val_accuracy: 0.3789\n",
      "Epoch 5/25\n",
      "237/237 [==============================] - 1412s 6s/step - loss: 4.1987 - accuracy: 0.3755 - val_loss: 4.2479 - val_accuracy: 0.3815\n",
      "Epoch 6/25\n",
      "237/237 [==============================] - 1413s 6s/step - loss: 4.1527 - accuracy: 0.3790 - val_loss: 4.2464 - val_accuracy: 0.3817\n",
      "Epoch 7/25\n",
      "237/237 [==============================] - 1413s 6s/step - loss: 4.1285 - accuracy: 0.3798 - val_loss: 4.2270 - val_accuracy: 0.3816\n",
      "Epoch 8/25\n",
      "237/237 [==============================] - 1413s 6s/step - loss: 4.0589 - accuracy: 0.3829 - val_loss: 4.2232 - val_accuracy: 0.3818\n",
      "Epoch 9/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.9891 - accuracy: 0.3905 - val_loss: 4.2241 - val_accuracy: 0.3776\n",
      "Epoch 10/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.9576 - accuracy: 0.3935 - val_loss: 4.2321 - val_accuracy: 0.3812\n",
      "Epoch 11/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.9163 - accuracy: 0.3986 - val_loss: 4.2214 - val_accuracy: 0.3826\n",
      "Epoch 12/25\n",
      "237/237 [==============================] - 1414s 6s/step - loss: 3.8778 - accuracy: 0.4038 - val_loss: 4.2035 - val_accuracy: 0.3809\n",
      "Epoch 13/25\n",
      "237/237 [==============================] - 1412s 6s/step - loss: 3.8411 - accuracy: 0.4073 - val_loss: 4.2179 - val_accuracy: 0.3789\n",
      "Epoch 14/25\n",
      "237/237 [==============================] - 1410s 6s/step - loss: 3.8106 - accuracy: 0.4111 - val_loss: 4.2611 - val_accuracy: 0.3663\n",
      "Epoch 15/25\n",
      "237/237 [==============================] - 1413s 6s/step - loss: 3.7660 - accuracy: 0.4157 - val_loss: 4.2556 - val_accuracy: 0.3775\n",
      "Epoch 16/25\n",
      "237/237 [==============================] - 1418s 6s/step - loss: 3.6809 - accuracy: 0.4276 - val_loss: 4.2750 - val_accuracy: 0.3763\n",
      "Epoch 17/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.7060 - accuracy: 0.4228 - val_loss: 4.2923 - val_accuracy: 0.3713\n",
      "Epoch 18/25\n",
      "237/237 [==============================] - 1415s 6s/step - loss: 3.6693 - accuracy: 0.4268 - val_loss: 4.3118 - val_accuracy: 0.3721\n",
      "Epoch 19/25\n",
      "237/237 [==============================] - 1407s 6s/step - loss: 3.6333 - accuracy: 0.4319 - val_loss: 4.3425 - val_accuracy: 0.3849\n",
      "Epoch 20/25\n",
      "237/237 [==============================] - 1419s 6s/step - loss: 3.6096 - accuracy: 0.4340 - val_loss: 4.3237 - val_accuracy: 0.3764\n",
      "Epoch 21/25\n",
      "237/237 [==============================] - 1416s 6s/step - loss: 3.5787 - accuracy: 0.4360 - val_loss: 4.3757 - val_accuracy: 0.3831\n",
      "Epoch 22/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.5595 - accuracy: 0.4374 - val_loss: 4.3981 - val_accuracy: 0.3809\n",
      "Epoch 23/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.5234 - accuracy: 0.4411 - val_loss: 4.4095 - val_accuracy: 0.3728\n",
      "Epoch 24/25\n",
      "237/237 [==============================] - 1416s 6s/step - loss: 3.4874 - accuracy: 0.4444 - val_loss: 4.4264 - val_accuracy: 0.3758\n",
      "Epoch 25/25\n",
      "237/237 [==============================] - 1411s 6s/step - loss: 3.4893 - accuracy: 0.4418 - val_loss: 4.4210 - val_accuracy: 0.3725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_27_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_27_layer_call_fn, lstm_cell_28_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: final_lstm_model_de-en/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: final_lstm_model_de-en/assets\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model(preproc_german_sentences.shape, preproc_english_sentences.shape[1], \n",
    "                   len(german_tokenizer.word_index)+1, len(english_tokenizer.word_index)+1)\n",
    "model.summary()\n",
    "model.fit(preproc_german_sentences, preproc_english_sentences, batch_size=50, epochs=25, validation_split=0.2, verbose=1)\n",
    "model.save('final_lstm_model_de-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "this is a the the the the the the the the the the the of .    \n",
      "\n",
      "Correct Translation:\n",
      "If the House agrees , I shall do as Mr Evans has suggested .\n",
      "\n",
      "Original text:\n",
      "Wenn das Haus damit einverstanden ist , werde ich dem Vorschlag von Herrn Evans folgen .\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "this is a the the the the .            \n",
      "\n",
      "Correct Translation:\n",
      "Madam President , on a point of order .\n",
      "\n",
      "Original text:\n",
      "Frau Präsidentin , zur Geschäftsordnung .\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "i , , , , , the the the the the the the the the the the the . .\n",
      "\n",
      "Correct Translation:\n",
      "I would like your advice about Rule 143 concerning inadmissibility .\n",
      "\n",
      "Original text:\n",
      "Könnten Sie mir eine Auskunft zu Artikel 143 im Zusammenhang mit der Unzulässigkeit geben ?\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "but , , the the the the the the the the . .       \n",
      "\n",
      "Correct Translation:\n",
      "My question relates to something that will come up on Thursday and which I will then raise again .\n",
      "\n",
      "Original text:\n",
      "Meine Frage betrifft eine Angelegenheit , die am Donnerstag zur Sprache kommen wird und auf die ich dann erneut verweisen werde .\n",
      "\n",
      "\n",
      "\n",
      "Prediction:\n",
      "that is therefore the is to to to the the the the the the the the . .  \n",
      "\n",
      "Correct Translation:\n",
      "The Cunha report on multiannual guidance programmes comes before Parliament on Thursday and contains a proposal in paragraph 6 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually .\n",
      "\n",
      "Original text:\n",
      "Das Parlament wird sich am Donnerstag mit dem Cunha-Bericht über mehrjährige Ausrichtungsprogramme befassen , der in Absatz 6 vorschlägt , daß Länder , die ihr Soll zur Flottenverkleinerung nicht erfüllen , jährlich mit einer Art Quotenstrafe belegt werden sollen .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,15):\n",
    "    # Print prediction(s)\n",
    "    print(\"Prediction:\")\n",
    "    print(logits_to_text(model.predict(preproc_german_sentences[i:i+1])[0], english_tokenizer))\n",
    "\n",
    "    print(\"\\nCorrect Translation:\")\n",
    "    print(' '.join([word for word in english_sentences[i:i+1][0]]))\n",
    "\n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(' '.join([word for word in german_sentences[i:i+1][0]]))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU Score\n",
    "Bilingual Evaluation Understudy Score - a metric for evaluating a generated sentence to a reference sentence. The scoring was developed for evaluating the prediction made by automatic machine translation systems. Score ranges 0 (exact mismatch) to 1 (exact match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GER:  Es gibt auch Beschlüsse gegen eine solche Steuer .\n",
      "ENG:  Decisions have also been adopted against a tax of this kind .\n",
      "Pred:  in is of the the the the of . .          \n",
      "BLEU score: 1.2803018097424153e-231\n",
      "\n",
      "GER:  Deswegen beantragt meine Fraktion , diesen Punkt von der Tagesordnung abzusetzen .\n",
      "ENG:  That is why my Group moves that this item be taken off the agenda .\n",
      "Pred:  i course , the , to . . . .          \n",
      "BLEU score: 1.3135841289152546e-231\n",
      "\n",
      "GER:  Vielen Dank , Herr Poettering .\n",
      "ENG:  Thank you , Mr Poettering .\n",
      "Pred:  the the of the the the the the the the the the the the the the the the . .\n",
      "BLEU score: 9.72161026064145e-232\n",
      "\n",
      "GER:  Wir kommen nun zu Herrn Wurtz , der gegen den Antrag spricht .\n",
      "ENG:  We shall now hear Mr Wurtz speaking against this request .\n",
      "Pred:  the is is is the the the the the the the the . .      \n",
      "BLEU score: 1.0931616654031189e-231\n",
      "\n",
      "GER:  Frau Präsidentin , ich möchte zunächst darauf hinweisen , daß das , was Herr Poettering da sagt , nicht ganz logisch ist .\n",
      "ENG:  Madam President , I would firstly like to point out Mr Poettering ' s lack of logic .\n",
      "Pred:  mr is , , , to to the . .          \n",
      "BLEU score: 1.331960397810445e-231\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(100,105):\n",
    "    original = ' '.join([word for word in german_sentences[i:i+1][0]])\n",
    "    references = ' '.join([word for word in english_sentences[i:i+1][0]])\n",
    "    candidates = logits_to_text(model.predict(preproc_german_sentences[i:i+1])[0], english_tokenizer)\n",
    "    score = sentence_bleu(references, candidates)\n",
    "    print('\\nGER: ', original)\n",
    "    print('ENG: ', references)\n",
    "    print('Pred: ', candidates)\n",
    "    print('BLEU score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest bleu score: 1.4773652796063933e-231\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "for i in range(0,len(preproc_german_sentences)):\n",
    "    references = ' '.join([word for word in english_sentences[i:i+1][0]])\n",
    "    candidates = logits_to_text(model.predict(preproc_german_sentences[i:i+1])[0], english_tokenizer)\n",
    "    bleu = sentence_bleu(references, candidates)\n",
    "    if bleu > score:\n",
    "        score = bleu\n",
    "print('highest bleu score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Model Attempts (English to French practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(simple_rnn_model.summary())\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "new jersey est parfois calme en mois de mai et il est neigeux en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom RNN Model\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(256)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(256, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    model.summary()\n",
    "    model.fit(x, y, batch_size=1024, epochs=25, validation_split=0.2)\n",
    "    model.save('rnn_model_fit_round2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 256)           4333056   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               789504    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 20, 512)           1182720   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 20, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 22719)         23286975  \n",
      "=================================================================\n",
      "Total params: 30,117,567\n",
      "Trainable params: 30,117,567\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "11/11 [==============================] - 1227s 112s/step - loss: 7.8674 - accuracy: 0.2185 - val_loss: 4.8652 - val_accuracy: 0.3568\n",
      "Epoch 2/25\n",
      "11/11 [==============================] - 1135s 103s/step - loss: 4.8177 - accuracy: 0.3409 - val_loss: 4.5789 - val_accuracy: 0.3568\n",
      "Epoch 3/25\n",
      "11/11 [==============================] - 1195s 106s/step - loss: 4.5294 - accuracy: 0.3467 - val_loss: 4.4383 - val_accuracy: 0.3679\n",
      "Epoch 4/25\n",
      "11/11 [==============================] - 1185s 108s/step - loss: 4.3747 - accuracy: 0.3567 - val_loss: 4.3322 - val_accuracy: 0.3770\n",
      "Epoch 5/25\n",
      "11/11 [==============================] - 1122s 102s/step - loss: 4.2515 - accuracy: 0.3643 - val_loss: 4.3111 - val_accuracy: 0.3753\n",
      "Epoch 6/25\n",
      "11/11 [==============================] - 1133s 103s/step - loss: 4.1577 - accuracy: 0.3692 - val_loss: 4.2999 - val_accuracy: 0.3755\n",
      "Epoch 7/25\n",
      "11/11 [==============================] - 1218s 112s/step - loss: 4.1069 - accuracy: 0.3708 - val_loss: 4.3025 - val_accuracy: 0.3785\n",
      "Epoch 8/25\n",
      "11/11 [==============================] - 1128s 103s/step - loss: 4.0660 - accuracy: 0.3725 - val_loss: 4.2696 - val_accuracy: 0.3734\n",
      "Epoch 9/25\n",
      "11/11 [==============================] - 1236s 114s/step - loss: 3.9930 - accuracy: 0.3779 - val_loss: 4.2903 - val_accuracy: 0.3790\n",
      "Epoch 10/25\n",
      "11/11 [==============================] - 1133s 103s/step - loss: 3.9283 - accuracy: 0.3814 - val_loss: 4.2814 - val_accuracy: 0.3749\n",
      "Epoch 11/25\n",
      "11/11 [==============================] - 1200s 110s/step - loss: 3.8856 - accuracy: 0.3864 - val_loss: 4.2826 - val_accuracy: 0.3680\n",
      "Epoch 12/25\n",
      "11/11 [==============================] - 1119s 102s/step - loss: 3.8570 - accuracy: 0.3871 - val_loss: 4.3776 - val_accuracy: 0.3814\n",
      "Epoch 13/25\n",
      "11/11 [==============================] - 1149s 105s/step - loss: 3.8323 - accuracy: 0.3889 - val_loss: 4.3260 - val_accuracy: 0.3715\n",
      "Epoch 14/25\n",
      "11/11 [==============================] - 1161s 105s/step - loss: 3.7440 - accuracy: 0.3983 - val_loss: 4.2886 - val_accuracy: 0.3718\n",
      "Epoch 15/25\n",
      "11/11 [==============================] - 1136s 103s/step - loss: 3.7329 - accuracy: 0.3954 - val_loss: 4.2912 - val_accuracy: 0.3702\n",
      "Epoch 16/25\n",
      "11/11 [==============================] - 1195s 109s/step - loss: 3.6543 - accuracy: 0.4043 - val_loss: 4.3057 - val_accuracy: 0.3820\n",
      "Epoch 17/25\n",
      "11/11 [==============================] - 1136s 103s/step - loss: 3.5931 - accuracy: 0.4073 - val_loss: 4.3197 - val_accuracy: 0.3619\n",
      "Epoch 18/25\n",
      "11/11 [==============================] - 1176s 107s/step - loss: 3.6042 - accuracy: 0.4043 - val_loss: 4.2974 - val_accuracy: 0.3812\n",
      "Epoch 19/25\n",
      "11/11 [==============================] - 1164s 106s/step - loss: 3.5157 - accuracy: 0.4097 - val_loss: 4.3309 - val_accuracy: 0.3847\n",
      "Epoch 20/25\n",
      "11/11 [==============================] - 1174s 107s/step - loss: 3.4280 - accuracy: 0.4170 - val_loss: 4.3340 - val_accuracy: 0.3768\n",
      "Epoch 21/25\n",
      "11/11 [==============================] - 1160s 105s/step - loss: 3.3905 - accuracy: 0.4186 - val_loss: 4.4075 - val_accuracy: 0.3825\n",
      "Epoch 22/25\n",
      "11/11 [==============================] - 1199s 109s/step - loss: 3.3161 - accuracy: 0.4248 - val_loss: 4.4649 - val_accuracy: 0.3861\n",
      "Epoch 23/25\n",
      "11/11 [==============================] - 1167s 106s/step - loss: 3.2583 - accuracy: 0.4282 - val_loss: 4.4181 - val_accuracy: 0.3716\n",
      "Epoch 24/25\n",
      "11/11 [==============================] - 1195s 109s/step - loss: 3.2288 - accuracy: 0.4278 - val_loss: 4.4679 - val_accuracy: 0.3821\n",
      "Epoch 25/25\n",
      "11/11 [==============================] - 1168s 106s/step - loss: 3.1857 - accuracy: 0.4304 - val_loss: 4.4885 - val_accuracy: 0.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses, gru_cell_8_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses, gru_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses, gru_cell_8_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses, gru_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_model_fit_round2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: rnn_model_fit_round2/assets\n"
     ]
    }
   ],
   "source": [
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 hours 20 min for round 1 --> rnn_model_fit\n",
    "# 8 hours 30 min for round 2 --> rnn_model_fit_round2 ( increased size of same layers and re-run )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "reprise de session <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[['Reprise', 'de', 'la', 'session']]\n",
      "\n",
      "Original text:\n",
      "[['Resumption', 'of', 'the', 'session']]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "rnn_model = load_model('rnn_model_fit_round2')\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def lstm_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"   \n",
    "    # Initialising the RNN\n",
    "    model = Sequential()\n",
    "    # word embedding layer seen before\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # repeat vector layer -- repeats the input n times, how many? \n",
    "    #model.add(RepeatVector(10))\n",
    "    # Adding a second LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    # output dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    # Compiling the RNN\n",
    "    model.compile(optimizer = 'adam', loss = sparse_categorical_crossentropy, metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 256)           4333056   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 50)            61400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 50)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 20, 22719)         1158669   \n",
      "=================================================================\n",
      "Total params: 5,573,325\n",
      "Trainable params: 5,573,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "11/11 [==============================] - 514s 45s/step - loss: 10.0219 - accuracy: 0.1930 - val_loss: 9.9519 - val_accuracy: 0.3568\n",
      "Epoch 2/25\n",
      "11/11 [==============================] - 507s 46s/step - loss: 9.8564 - accuracy: 0.3450 - val_loss: 9.3400 - val_accuracy: 0.3568\n",
      "Epoch 3/25\n",
      "11/11 [==============================] - 498s 45s/step - loss: 9.0966 - accuracy: 0.3414 - val_loss: 8.2519 - val_accuracy: 0.3568\n",
      "Epoch 4/25\n",
      "11/11 [==============================] - 496s 45s/step - loss: 7.9836 - accuracy: 0.3444 - val_loss: 7.2031 - val_accuracy: 0.3568\n",
      "Epoch 5/25\n",
      "11/11 [==============================] - 504s 46s/step - loss: 6.9969 - accuracy: 0.3437 - val_loss: 6.4130 - val_accuracy: 0.3568\n",
      "Epoch 6/25\n",
      "11/11 [==============================] - 501s 45s/step - loss: 6.2713 - accuracy: 0.3428 - val_loss: 5.8444 - val_accuracy: 0.3568\n",
      "Epoch 7/25\n",
      "11/11 [==============================] - 496s 45s/step - loss: 5.7600 - accuracy: 0.3424 - val_loss: 5.4424 - val_accuracy: 0.3568\n",
      "Epoch 8/25\n",
      "11/11 [==============================] - 517s 47s/step - loss: 5.3959 - accuracy: 0.3421 - val_loss: 5.1551 - val_accuracy: 0.3568\n",
      "Epoch 9/25\n",
      "11/11 [==============================] - 542s 49s/step - loss: 5.1558 - accuracy: 0.3402 - val_loss: 4.9669 - val_accuracy: 0.3568\n",
      "Epoch 10/25\n",
      "11/11 [==============================] - 534s 48s/step - loss: 4.9953 - accuracy: 0.3415 - val_loss: 4.8682 - val_accuracy: 0.3568\n",
      "Epoch 11/25\n",
      "11/11 [==============================] - 538s 48s/step - loss: 4.9197 - accuracy: 0.3419 - val_loss: 4.8289 - val_accuracy: 0.3568\n",
      "Epoch 12/25\n",
      "11/11 [==============================] - 498s 45s/step - loss: 4.8848 - accuracy: 0.3415 - val_loss: 4.8141 - val_accuracy: 0.3568\n",
      "Epoch 13/25\n",
      "11/11 [==============================] - 507s 46s/step - loss: 4.8757 - accuracy: 0.3408 - val_loss: 4.8076 - val_accuracy: 0.3568\n",
      "Epoch 14/25\n",
      "11/11 [==============================] - 505s 46s/step - loss: 4.8428 - accuracy: 0.3441 - val_loss: 4.8050 - val_accuracy: 0.3568\n",
      "Epoch 15/25\n",
      "11/11 [==============================] - 505s 46s/step - loss: 4.8570 - accuracy: 0.3409 - val_loss: 4.8039 - val_accuracy: 0.3568\n",
      "Epoch 16/25\n",
      "11/11 [==============================] - 512s 47s/step - loss: 4.8482 - accuracy: 0.3416 - val_loss: 4.8036 - val_accuracy: 0.3568\n",
      "Epoch 17/25\n",
      "11/11 [==============================] - 509s 46s/step - loss: 4.8326 - accuracy: 0.3435 - val_loss: 4.8031 - val_accuracy: 0.3568\n",
      "Epoch 18/25\n",
      "11/11 [==============================] - 512s 46s/step - loss: 4.8350 - accuracy: 0.3430 - val_loss: 4.8037 - val_accuracy: 0.3568\n",
      "Epoch 19/25\n",
      "11/11 [==============================] - 512s 46s/step - loss: 4.8338 - accuracy: 0.3424 - val_loss: 4.8042 - val_accuracy: 0.3568\n",
      "Epoch 20/25\n",
      "11/11 [==============================] - 511s 47s/step - loss: 4.8172 - accuracy: 0.3453 - val_loss: 4.8047 - val_accuracy: 0.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm_model_fit/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm_model_fit/assets\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model(preproc_english_sentences.shape, preproc_french_sentences.shape[1], \n",
    "                   len(english_tokenizer.word_index)+1, len(french_tokenizer.word_index)+1)\n",
    "model.summary()\n",
    "model.fit(preproc_english_sentences, preproc_french_sentences, batch_size=1024, epochs=25, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)], verbose=1)\n",
    "model.save('lstm_model_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[['Reprise', 'de', 'la', 'session']]\n",
      "\n",
      "Original text:\n",
      "[['Resumption', 'of', 'the', 'session']]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "\n",
      "Correct Translation:\n",
      "[['(', 'Le', 'Parlement', ',', 'debout', ',', 'observe', 'une', 'minute', 'de', 'silence', ')']]\n",
      "\n",
      "Original text:\n",
      "[['(', 'The', 'House', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(model.predict(tmp_x[4:5])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[4:5])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1024 batch, 20 epoch round 1 -- ~3 hours --> 'lstm_model_fit' 36% val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
